{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarathSabu/Python-Notebooks/blob/main/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==1.4.2"
      ],
      "metadata": {
        "id": "6bHyMJZnycML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import scikeras\n",
        "print(sklearn.__version__)\n",
        "print(scikeras.__version__)"
      ],
      "metadata": {
        "id": "F_xRWmEygcLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVZZQ-0QNfia"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.random import set_seed\n",
        "from random import seed\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SctxwKyUeI1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# import data as dataframe\n",
        "file_path = '/content/drive/MyDrive/Infor648/Data/churn_exam.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# calling head() method\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AqtQBRjrOJ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "kGI_d5YtPVLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BCxLVZ21PVvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "QCe4JWdZ8U0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.isna().sum()) ##check missing value"
      ],
      "metadata": {
        "id": "9CCqUPBsPYC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna() ##drop missing value\n",
        "display(df.isna().sum()) ##recheck missing value again"
      ],
      "metadata": {
        "id": "n95mhucCPbVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric Variables\n",
        "numeric_variables = [col for col in df.columns if df[col].dtype != \"object\" and col not in \"Customer Status\"] ##exclude our target variable: customer status\n",
        "numeric_variables"
      ],
      "metadata": {
        "id": "PjcREED_PeR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variables = [col for col in df.columns if df[col].dtype == \"O\" and col != \"Customer Status\"]  ###exclude our target: \"Customer Status\"\n",
        "categorical_variables"
      ],
      "metadata": {
        "id": "J5_HAOz-Pfqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Internet Type'].value_counts()"
      ],
      "metadata": {
        "id": "Vyv-LN_f1muj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Contract'].value_counts()"
      ],
      "metadata": {
        "id": "LjX_LhtF2pgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Offer'].value_counts()"
      ],
      "metadata": {
        "id": "vc7G6b6C3b0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select Variables of Interest"
      ],
      "metadata": {
        "id": "M2wK9MaM8eUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = df[[\"Number of Dependents\", \"Number of Referrals\",\"Total Long Distance Charges\",\"Total Extra Data Charges\",\"Gender\",\"Offer\",\"Unlimited Data\", \"Customer Status\"]]"
      ],
      "metadata": {
        "id": "EPPedvEvPhhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub"
      ],
      "metadata": {
        "id": "Jkp1uDEgTw_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##encode categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_sub['Gender'] = label_encoder.fit_transform(df_sub['Gender'])\n",
        "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "\n",
        "#Print out what we encoded for gender\n",
        "print(\"Gender Encoding:\")\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "84BIjkN3hsbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##encode categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_sub['Unlimited Data'] = label_encoder.fit_transform(df_sub['Unlimited Data'])\n",
        "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "\n",
        "#Print out what we encoded for gender\n",
        "print(\"Unlimited Data Encoding:\")\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "q0-XzxUATvMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-hot encoding"
      ],
      "metadata": {
        "id": "z0J_8i298plM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding transforms categorical variables into a set of binary columns (0s and 1s), one for each category.\n",
        "# This ensures no ordinal relationship is imposed between the categories (which could be problematic in label encoding).\n",
        "\n",
        "df_sub_encoded = pd.get_dummies(df_sub, columns=['Offer'])\n",
        "\n",
        "# Ensure that all binary columns are integers (0 and 1)\n",
        "df_sub_encoded = df_sub_encoded.astype({col: int for col in df_sub_encoded.columns if 'Offer' in col})\n",
        "\n",
        "\n",
        "\n",
        "df_sub_encoded"
      ],
      "metadata": {
        "id": "nZ1rYmN3OtUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encod our target variable"
      ],
      "metadata": {
        "id": "-JzRUn0b8034"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_sub_encoded['Customer Status'].value_counts())\n",
        "##Our target variable is a categorical variable"
      ],
      "metadata": {
        "id": "ySPCahbATd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Encode our target variable\n",
        "target_label_encoder = LabelEncoder()\n",
        "df_sub_encoded['Customer Status'] = target_label_encoder.fit_transform(df_sub_encoded['Customer Status'])\n",
        "\n",
        "\n",
        "##display the stats after encoding\n",
        "display(df_sub_encoded['Customer Status'].value_counts())\n",
        "mapping = dict(zip(target_label_encoder.classes_, target_label_encoder.transform(target_label_encoder.classes_)))\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "RIVHvCt1TivB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_sub_encoded.corr()\n",
        "plt.figure(figsize=(9,9)) ###change the figure size here\n",
        "sns.heatmap(corr_matrix, cmap='Blues', annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VTnDsztNMts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split into test and training dataset"
      ],
      "metadata": {
        "id": "lPP9SXLN9XpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_sub_encoded.drop('Customer Status', axis=1)  # Drop the target column to get independent variables\n",
        "y = df_sub_encoded['Customer Status']  # Select the target column directly as our y\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets test_size using 0.3: 70% training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "XtdoCEoUUQEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "etMezHe6Uy0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "QwH8hheVWdRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization or standardization"
      ],
      "metadata": {
        "id": "2n62SmPRYqGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#data standardization\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "z1peeaaTUyH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.preprocessing import MinMaxScaler\n",
        "#data normalization\n",
        "#nc = MinMaxScaler()\n",
        "#X_train = nc.fit_transform(X_train)\n",
        "#X_test = nc.transform(X_test)"
      ],
      "metadata": {
        "id": "jZQG4CvSYcPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##you do not need to do anything here\n",
        "def nn_train(num_layers = 2, num_neurons = 8, opt='adam', dropout_rate=0.0, seed_value=42, input_shape=None):\n",
        "    \"\"\"\n",
        "    Build and train a neural network for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape: Input features for training.\n",
        "    num_layers (int): Total number of hidden layers (excluding input/output layers).\n",
        "    num_neurons (int): Number of neurons in each hidden layer.\n",
        "    opt (str): Optimizer to use (default: 'adam').\n",
        "    seed_value (int): Random seed value for reproducibility.\n",
        "    dropout_rate (float): Fraction of input units to drop, between 0 and 1 (default: 0).\n",
        "    randomly \"drops\" a fraction of the neurons' outputs in the layer it's applied to during each training step. This helps prevent overfitting\n",
        "    \"\"\"\n",
        "    K.clear_session()\n",
        "    # Set random seed for reproducibility\n",
        "    seed(seed_value)\n",
        "    set_seed(seed_value)\n",
        "\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add an Input layer to specify the input shape\n",
        "    model.add(Input(shape=(input_shape,)))\n",
        "\n",
        "    # Add additional hidden layers, all with `num_neurons` neurons\n",
        "    for _ in range(num_layers):\n",
        "      model.add(Dense(num_neurons, activation='relu'))\n",
        "      if dropout_rate > 0:  # Add dropout only if rate is greater than 0\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer with 1 neuron for binary classification using sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid')) ###change this to softmax for multi-class and change the number here to align with the number of class\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) #change to categorical_crossentropy\n",
        "\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "AkZZdFAta11h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num_layers (int): Total number of hidden layers (excluding input/output layers).\n",
        "#num_neurons (int): Number of neurons in each hidden layer.\n",
        "nn_model = nn_train(num_layers=3, input_shape=X_train.shape[1], num_neurons=8, opt='adam', seed_value=42) #You can heuristically set the number of neurons in each hidden layer to be 2 times the input dimension"
      ],
      "metadata": {
        "id": "G6r4iyJljLRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn_model.summary())\n",
        "\n",
        "##parameter calculation = #weights(#Input * #neurons) + #bias\n",
        "#4 bytes per parameter"
      ],
      "metadata": {
        "id": "G6fK1PvNmIFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train: Input features for training.\n",
        "#y_train: Target variable for training.\n",
        "\n",
        "training = nn_model.fit(X_train, y_train, epochs=50, batch_size=30, validation_split=0.2) #split into validation set to evaluate how well the model performs on unseen data during training.\n",
        "#Batch Size: 30 means that the model will update its parameters after every 30 sample(every batch)\n",
        "#Epoch: meaning the model will go through the entire dataset 50 times."
      ],
      "metadata": {
        "id": "9AL5XMkwpST4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_accurancy = np.mean(training.history['val_accuracy'])\n",
        "print(\"\\n%s: %.2f%%\" % ('validation_accurancy', validation_accurancy*100))"
      ],
      "metadata": {
        "id": "fe4DYOIawRSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##You do not need to do anything here\n",
        "def plot_training_history(training):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(training.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(training.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(training.history['loss'], label='Train Loss')\n",
        "    plt.plot(training.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "juGnz4gGsqKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(training)"
      ],
      "metadata": {
        "id": "nMYpVXKitp3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tunig"
      ],
      "metadata": {
        "id": "uKWHKbSmBRRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the function with KerasClassifier for grid search\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "model = KerasClassifier(model=nn_train, input_shape=X_train.shape[1], verbose=0)\n",
        "#model = KerasClassifier(model=nn_train, n_features=X_train.shape[1], verbose=0)\n",
        "batch_size = [20, 30, 60] ##change the batch size here\n",
        "epochs = [30, 50, 60]  #change the number of epochs here\n",
        "#if you select more than 3 numbers, it will take some time to train\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Perform grid search using GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose = 2) #you can choose cv as 3 or 5\n",
        "grid_result_batch = grid_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "N3W8aPIjvR1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize results\n",
        "print(f\"Best parameters: {grid_result_batch.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_result_batch.best_score_}\")\n",
        "means = grid_result_batch.cv_results_['mean_test_score']\n",
        "stds = grid_result_batch.cv_results_['std_test_score']\n",
        "params = grid_result_batch.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "xiZPAYyvEOGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KerasClassifier(model=nn_train, input_shape=X_train.shape[1], num_layers=None, num_neurons=None,\n",
        "                        epochs=50, batch_size=60, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "num_layers = [1, 2, 3]  # Different number of hidden layers (less than 5)\n",
        "num_neurons =  [8, 16, 22]  # Different number of neurons, you can try 2*input dimension\n",
        "\n",
        "param_grid = dict(num_layers=num_layers, num_neurons = num_neurons)\n",
        "\n",
        "# search the grid\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv = 3, verbose=2) ##cv cross-validation 3-fold or 5-fold\n",
        "grid_result_layer = grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HHEXbDWbEhEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize results\n",
        "print(f\"Best parameters: {grid_result_layer.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_result_layer.best_score_}\")\n",
        "means = grid_result_layer.cv_results_['mean_test_score']\n",
        "stds = grid_result_layer.cv_results_['std_test_score']\n",
        "params = grid_result_layer.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "I7qrue6K-Dzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = KerasClassifier(model=nn_train, input_shape=X_train.shape[1], num_layers=2, num_neurons=16, dropout_rate= None,\n",
        "                        epochs=50, batch_size=60, verbose=0)\n",
        "# define the grid search parameters\n",
        "dropout_rates = [0.0, 0.2, 0.3, 0.5]\n",
        "\n",
        "param_grid_dropout = dict(dropout_rate = dropout_rates)\n",
        "\n",
        "grid_dropout = GridSearchCV(estimator=model, param_grid=param_grid_dropout, cv=3, verbose=2)\n",
        "grid_result_dropout = grid_dropout.fit(X_train, y_train)\n",
        "\n",
        "# Get the best dropout rate\n",
        "best_dropout_rate = grid_result_dropout.best_params_['dropout_rate']"
      ],
      "metadata": {
        "id": "V-gkf_c_ANwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize results\n",
        "print(f\"Best parameters: {grid_result_dropout.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_result_dropout.best_score_}\")\n",
        "means = grid_result_dropout.cv_results_['mean_test_score']\n",
        "stds = grid_result_dropout.cv_results_['std_test_score']\n",
        "params = grid_result_dropout.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "JB5rcvalBzTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create final model\n",
        "final_model = nn_train(num_layers=2, input_shape=X_train.shape[1], num_neurons=16, opt='adam', dropout_rate = 0.0, seed_value=42)\n",
        "\n",
        "final_model.summary()"
      ],
      "metadata": {
        "id": "ftn9ztaHHSbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_training = final_model.fit(X_train, y_train, epochs=50, batch_size=60,\n",
        "                     validation_split=0.2, verbose=2)"
      ],
      "metadata": {
        "id": "_9limftpHUb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "validation_accurancy_final = np.mean(final_training.history['val_accuracy'])\n",
        "print(\"\\n%s: %.2f%%\" % ('validation_accurancy_final', validation_accurancy_final*100))\n",
        "print(\"\\n%s: %.2f%%\" % ('validation_accurancy_previous', validation_accurancy*100))"
      ],
      "metadata": {
        "id": "SpS00J-6JWOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(final_training)"
      ],
      "metadata": {
        "id": "Sp7pU3pBHtzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = final_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FKodzXPYJykl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "fFY0OjZWKfmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[y_pred >0.5] = 1\n",
        "y_pred[y_pred <= 0.5] = 0\n",
        "y_pred"
      ],
      "metadata": {
        "id": "EMsxPNBbKl0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a DataFrame for evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame({\n",
        "    \"Evaluation Metric\": [\"Train Accuracy\", \"Test Accuracy\", \"Recall\", \"Precision\", \"F1 Score\"],\n",
        "    \"Value\": [\n",
        "        validation_accurancy_final * 100,\n",
        "        accuracy_score(y_test, y_pred) * 100,\n",
        "        recall_score(y_test, y_pred) * 100,\n",
        "        precision_score(y_test, y_pred) * 100,\n",
        "        f1_score(y_test, y_pred) * 100\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "display(evaluation_metrics)\n"
      ],
      "metadata": {
        "id": "-p4AYBWJLeoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size here\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lOZF8s51MuiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "EQUAmsgKPQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a consumer with 2 dependents, 0 referral and 50.31 total long distance charges, 0 total extra data charges, male and yes for unlimited data and offered offer A"
      ],
      "metadata": {
        "id": "6rTl-hRI7uCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input data for a single prediction (make sure it matches the encoding of your training data)\n",
        "single_input = np.array([[2, 0, 50.31, 0, 1, 1, 1, 0, 0, 0, 0]])  # Replace these values with your actual input\n",
        "\n",
        "\n",
        "single_input_scaled = sc.transform(single_input)\n",
        "# Make a single prediction\n",
        "single_prediction = final_model.predict(single_input_scaled)\n"
      ],
      "metadata": {
        "id": "jFHR5r4WPD-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_prediction"
      ],
      "metadata": {
        "id": "16PBxVSmOWQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_prediction = (single_prediction > 0.5).astype(\"int32\")\n",
        "\n",
        "print(f\"Predicted class for the input: {single_prediction[0][0]}\")"
      ],
      "metadata": {
        "id": "tL8up9tMPhR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}