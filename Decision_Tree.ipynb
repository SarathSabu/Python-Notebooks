{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarathSabu/Python-Notebooks/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "# For data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For model training and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "%matplotlib inline\n",
        "\n",
        "# For Google Colab integration\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "KpTTwaImyNLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the Data and Check the Stats & Columns"
      ],
      "metadata": {
        "id": "W3EOxtuiG0_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data as dataframe\n",
        "file_path = '/content/drive/MyDrive/Infor648/Data/churn.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# calling head() method\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FR-C7amcy-Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "HnxRaeqoguux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "kbs_UerNG-jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Rows with a Missing Value"
      ],
      "metadata": {
        "id": "xfOc07F5zSDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.isna().sum()) ##check missing value"
      ],
      "metadata": {
        "id": "PV0ziJGVgbF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiple Ways to Handle Missing Values in a Dataset"
      ],
      "metadata": {
        "id": "seQN1jNQbJWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1 Drop them directly"
      ],
      "metadata": {
        "id": "sTAIY8hEbamL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##drop them directly\n",
        "\n",
        "df = df.dropna() ##drop missing value"
      ],
      "metadata": {
        "id": "LZ2koTbFgm51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2 Do a quick check"
      ],
      "metadata": {
        "id": "I4Q3mWi1bdeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = [\n",
        "    \"Offer\", \"Avg Monthly Long Distance Charges\", \"Multiple Lines\",\n",
        "    \"Internet Type\", \"Avg Monthly GB Download\", \"Online Security\",\n",
        "    \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
        "    \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\", \"Unlimited Data\"\n",
        "]\n",
        "\n",
        "# Filter DataFrame to show only columns with missing values\n",
        "df[missing_cols]"
      ],
      "metadata": {
        "id": "qSd7yXv2aJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Avg Monthly Long Distance Charges\"] = df[\"Avg Monthly Long Distance Charges\"].fillna(df[\"Avg Monthly Long Distance Charges\"].mean())\n",
        "df[\"Avg Monthly GB Download\"] = df[\"Avg Monthly GB Download\"].fillna(df[\"Avg Monthly GB Download\"].mean())\n"
      ],
      "metadata": {
        "id": "0X0vf5lBbrPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"Gender\", \"Age\", \"Married\", \"Number of Dependents\", \"Tenure in Months\", \"Monthly Charge\", \"Total Charges\", \"Total Refunds\" ,\"Total Revenue\",\"Internet Type\",\"Customer Status\"]]"
      ],
      "metadata": {
        "id": "u7-3kbK2bGa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.isna().sum()) ##recheck missing value again"
      ],
      "metadata": {
        "id": "AbZFjv80AfeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna() ##drop missing value"
      ],
      "metadata": {
        "id": "exZlW_zXdLeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.isna().sum()) ##recheck missing value again"
      ],
      "metadata": {
        "id": "y8_HDANJgpzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now lets check how many categorical/numeric variables we have"
      ],
      "metadata": {
        "id": "4nvHgQS7xGif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric Variables\n",
        "numeric_variables = [col for col in df.columns if df[col].dtype != \"object\" and col not in \"Customer Status\"] ##exclude our target variable: customer status\n",
        "numeric_variables"
      ],
      "metadata": {
        "id": "h93w__B2REru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variables = [col for col in df.columns if df[col].dtype == \"O\" and col != \"Customer Status\"]  ###exclude our target: \"Customer Status\"\n",
        "categorical_variables"
      ],
      "metadata": {
        "id": "T2fF8DFXgBVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Married']"
      ],
      "metadata": {
        "id": "0yYYvl1kKegr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender']"
      ],
      "metadata": {
        "id": "BprbmcEggsZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Internet Type']"
      ],
      "metadata": {
        "id": "XRpuKC-hRUrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What is your recommendation ?\n",
        "\n",
        "###The company seeks to understand the factors that influence a consumer's decision to stay or leave their service and aims to predict customer churn.\n",
        "\n",
        "###The company hopes to take proactive steps to retain customers before they decide to leave.\n",
        "\n",
        "###Initially, they believe factors such as family status, monthly charges, and length of time with the service may play a role, and they are looking to explore these and other potential predictors."
      ],
      "metadata": {
        "id": "GOzldYfWmBL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select the variables we are interested in"
      ],
      "metadata": {
        "id": "_tymrhY6HkST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = df[[\"Gender\", \"Age\", \"Married\", \"Number of Dependents\", \"Tenure in Months\", \"Monthly Charge\", \"Total Charges\", \"Total Refunds\" ,\"Total Revenue\",\"Internet Type\",\"Customer Status\"]]"
      ],
      "metadata": {
        "id": "nqMKNdw3hANb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encode our categorical data\n",
        "\n",
        "####Do not click the encoding several times. If it does not show the category as\n",
        "####{'Female': 0, 'Male': 1}\n",
        "####but as {0: 0, 1: 1}\n",
        "####you need to start from the select variables steps again df_sub = df[[\"Gender\", \"Age\", \"Married\", \"Number of Dependents\", \"Tenure in Months\", \"Monthly Charge\", \"Total Charges\", \"Total Refunds\" ,\"Total Revenue\",\"Internet Type\",\"Customer Status\"]]"
      ],
      "metadata": {
        "id": "Aqo7nV5VIi3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##encode categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_sub['Gender'] = label_encoder.fit_transform(df_sub['Gender'])\n",
        "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "\n",
        "#Print out what we encoded for gender\n",
        "print(\"Gender Encoding:\")\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "izP9CqILnQRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub['Married'] = label_encoder.fit_transform(df_sub['Married'])\n",
        "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "\n",
        "#prints out what we encoded for married\n",
        "print(\"Married Encoding:\")\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "aQJpUsL-wQtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[['Married','Gender']]"
      ],
      "metadata": {
        "id": "bbpTZTUHzMSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-hot encoding\n",
        "\n",
        "## One-hot encoding transforms categorical variables into a set of binary columns (0s and 1s), one for each category.\n",
        "## This ensures no ordinal relationship is imposed between the categories (which could be problematic in label encoding)."
      ],
      "metadata": {
        "id": "3_Yv_uVuTmN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = pd.get_dummies(df_sub, columns=['Internet Type'])"
      ],
      "metadata": {
        "id": "9LQb3c7hQuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub"
      ],
      "metadata": {
        "id": "h1C8Qv1CRh2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_sub.columns = df_sub.columns.str.replace(' ', '_')"
      ],
      "metadata": {
        "id": "Ux4UfuSA5qAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now lets check our target variable"
      ],
      "metadata": {
        "id": "RMf_EB4ZlxtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_sub['Customer Status'].value_counts())\n",
        "##Our target variable is a categorical variable"
      ],
      "metadata": {
        "id": "bWE2zfr7i5Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub['Customer Status']"
      ],
      "metadata": {
        "id": "T6QwtS1AR29U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####We are only interested in why people stayed and churned\n",
        "df_sub = df_sub[df_sub['Customer Status'] !='Joined'] # we drop all the new customers\n",
        "\n",
        "####Encode our target variable\n",
        "target_label_encoder = LabelEncoder()\n",
        "df_sub['Customer Status'] = target_label_encoder.fit_transform(df_sub['Customer Status'])\n",
        "\n",
        "\n",
        "##display the stats after encoding\n",
        "display(df_sub['Customer Status'].value_counts())\n",
        "mapping = dict(zip(target_label_encoder.classes_, target_label_encoder.transform(target_label_encoder.classes_)))\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "OzmxDwxPjDyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let see how each feature is related to our target variable"
      ],
      "metadata": {
        "id": "SlBu1qkpmZyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_sub.corr()\n",
        "plt.figure(figsize=(9,9)) ###change the figure size here\n",
        "sns.heatmap(corr_matrix, cmap='Blues', annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R99q_zkY3j1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define features (X) and target (y), then split the data into training/testing sets\n"
      ],
      "metadata": {
        "id": "hhVpwecqKX3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_sub.drop('Customer Status', axis=1)  # Drop the target column to get independent variables\n",
        "y = df_sub['Customer Status']  # Select the target column directly as our y\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets test_size using 0.3: 70% training and 30% testing\n",
        "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "id": "jbjK4ZT_ENw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "eZfvFwQnEwZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "BEU7EDJihm3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "AHx-mkCxhqDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "PH2ytLWBhras"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train our first decision tree model"
      ],
      "metadata": {
        "id": "bkjEgizWLJ5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#criterion='entropy': Specifies that the decision tree should use entropy to measure the quality of a split.\n",
        "#max_depth=12: Limits the maximum depth of the tree to 12 levels.\n",
        "#min_samples_split=5: Specifies that a node must have at least 5 samples to be considered for splitting\n",
        "\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_split = 5, random_state=1)\n",
        "\n",
        "\n",
        "###train the model\n",
        "dt_clf = dt_clf.fit(X_train, y_train)\n",
        "\n",
        "###making predictions on test data\n",
        "y_pred = dt_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "_Q2FjMWUhubS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####run this step to mark the features to the plot we are going to generate\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Automatically get all unique target classes\n",
        "##It should give you the target label before we encode it\n",
        "\n",
        "class_names = target_label_encoder.inverse_transform(np.arange(len(target_label_encoder.classes_)))\n",
        "\n",
        "##print out the features we selected for predictions and our classification target\n",
        "print(\"features:\",feature_names)\n",
        "print(\"Classes:\", class_names)"
      ],
      "metadata": {
        "id": "8r-ZmCfvVRz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Text representation of our trained Decision Tree\n",
        "from sklearn.tree import export_text\n",
        "feature_names = X_train.columns.tolist()\n",
        "text_representation = export_text(dt_clf, feature_names=feature_names)\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "RBvsxfOPGkGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Figure visualization of our trained Decision Tree\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "\n",
        "# Generate the DOT data for the tree\n",
        "dot_data = export_graphviz(dt_clf,\n",
        "                           out_file=None,\n",
        "                           feature_names=feature_names,\n",
        "                           class_names=class_names,\n",
        "                           filled=True,\n",
        "                           rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create the Graphviz source object\n",
        "decision_tree_graph = graphviz.Source(dot_data, format=\"png\")\n",
        "decision_tree_graph.render(\"decision_tree_graph\")\n",
        "# Display the decision tree within the notebook\n",
        "display(Image(filename=\"decision_tree_graph.png\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ed_5JC4wJ0mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Uncomment this only if you want to save your decision tree image\n",
        "##You need to create a folder called Image under My Drive first\n",
        "#image_folder_path = '/content/drive/My Drive/Image/'\n",
        "#decision_tree_graph.render(image_folder_path + \"decision_tree_graph\")"
      ],
      "metadata": {
        "id": "QhcWNGTLir6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now lets evaluate our first decision tree"
      ],
      "metadata": {
        "id": "HmTmttJBNidX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "evaluation_metrics = pd.DataFrame({\n",
        "    \"Evaluation Metric\": [\"Train Accuracy\", \"Test Accuracy\", \"Recall\", \"Precision\", \"F1 Score\"],\n",
        "    \"Value\": [\n",
        "        dt_clf.score(X_train, y_train),\n",
        "        accuracy_score(y_test, y_pred),\n",
        "        recall_score(y_test, y_pred),\n",
        "        precision_score(y_test, y_pred),\n",
        "        f1_score(y_test, y_pred)\n",
        "    ]\n",
        "})\n",
        "\n",
        "evaluation_metrics"
      ],
      "metadata": {
        "id": "6Llpouq2NKp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))###change this number to adjust figure size\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mkFpliSp9zSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Generate the evaluation figure for all the classes\n",
        "from yellowbrick.model_selection import ValidationCurve\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "visualizer = ClassificationReport(dt_clf, classes=class_names, support=False, title = \"Decision Tree Classifier Evaluation\")\n",
        "visualizer.fit(X_train, y_train)\n",
        "visualizer.score(X_test, y_test)\n",
        "\n",
        "visualizer.ax.set_xticklabels(['Precision', 'Recall', 'F1'])\n",
        "visualizer.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Iv2nDPPDKL19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature importance for predicting target"
      ],
      "metadata": {
        "id": "f1tVeL60QF8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The feature importance values in a decision tree model indicate how much a particular feature contributes to the prediction of the target variable.\n",
        "#The importance score is calculated based on how often and how effectively a feature is used to split the data\n",
        "#Features that are used to split the data closer to the root of the tree or that result in greater reductions in entropy will have higher importance scores.\n",
        "#The importance scores are typically normalized, meaning they sum up to 1 (or 100% when expressed as percentages).\n",
        "#A higher score indicates a more important feature in determining the outcome of the target variable.\n",
        "\n",
        "\n",
        "# Retrieve the feature importances from the trained model\n",
        "feature_importances = dt_clf.feature_importances_\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting the feature importances\n",
        "plt.figure(figsize=(8, 5))###########change this number to adjust figure size\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "\n",
        "for index, value in enumerate(feature_importance_df['Importance']):\n",
        "    plt.text(value, index, f'{value:.3f}', va='center')    #.3f means the value is round up to 3 decimal places\n",
        "\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.gca().invert_yaxis()  # To have the most important feature at the top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R0VKhDBvNPgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "23JXNAcnQ7hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Validation Evaluation on the training set"
      ],
      "metadata": {
        "id": "WRx8KYX5SjpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "\n",
        "\n",
        "# Perform cross-validation and get aggregated predictions across the entire training set\n",
        "y_pred_cross = cross_val_predict(dt_clf, X_train, y_train, cv=5)\n",
        "\n",
        "\n",
        "# Cross-Validation Results on Training Set\n",
        "y_pred_cross = cross_val_predict(dt_clf, X_train, y_train, cv=5)\n",
        "\n",
        "accuracy_cv = accuracy_score(y_train, y_pred_cross)\n",
        "recall_cv = recall_score(y_train, y_pred_cross)\n",
        "precision_cv = precision_score(y_train, y_pred_cross)\n",
        "f1_cv = f1_score(y_train, y_pred_cross)\n",
        "matrix_cv = confusion_matrix(y_train, y_pred_cross)\n",
        "\n",
        "# Create DataFrame for evaluation metrics with cross-validation\n",
        "evaluation_metrics_with_cv = pd.DataFrame({\n",
        "    \"Evaluation Metric_CV\": [\"Train Accuracy\", \"Test Accuracy\", \"Recall\", \"Precision\", \"F1 Score\"],\n",
        "    \"Value\": [\n",
        "        accuracy_cv,\n",
        "        accuracy_cv,  # Cross-validation doesn't separate train/test, so the same accuracy is used\n",
        "        recall_cv,\n",
        "        precision_cv,\n",
        "        f1_cv\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Performance Metrics With Cross-Validation:\")\n",
        "display(evaluation_metrics_with_cv)\n",
        "print('\\nConfusion Matrix CV:','\\n', matrix_cv)\n",
        "\n",
        "\n",
        "print(\"\\nPerformance Metrics Without Cross-Validation:\")\n",
        "display(evaluation_metrics)\n",
        "print('\\nConfusion Matrix:','\\n', conf_matrix)\n",
        "\n",
        "\n",
        "## Alternatively, use cross_val_score to get the average performance metrics across folds/different data splits\n",
        "# scores_acc = cross_val_score(dt_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "#scores_rc = cross_val_score(dt_clf, X_train, y_train, cv=5, scoring='recall')\n",
        "#scores_prc = cross_val_score(dt_clf, X_train, y_train, cv=5, scoring='precision')\n",
        "#scores_f1 = cross_val_score(dt_clf, X_train, y_train, cv=5, scoring='f1_macro')\n",
        "\n",
        "#print('\\nCross-Validation Scores on Training Set:')\n",
        "#print('Average Accuracy: ', scores_acc.mean())\n",
        "#print('Average Recall: ', scores_rc.mean())\n",
        "#print('Average Precision: ', scores_prc.mean())\n",
        "#print('Average F1-score: ', scores_f1.mean())\n"
      ],
      "metadata": {
        "id": "dpyIWLoXjuJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Fine-tuning (Pruning)"
      ],
      "metadata": {
        "id": "XpCrTEQieDo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of max_depth values to evaluate\n",
        "max_depth_range = range(3, 10)  # You can adjust the range as needed\n",
        "\n",
        "# Lists to store performance metrics\n",
        "train_accuracies = []\n",
        "cv_test_accuracies = []\n",
        "\n",
        "for depth in max_depth_range:\n",
        "    # Initialize the model with the current max_depth\n",
        "    dt_depth = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=1)\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    dt_depth.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    y_pred_train = dt_depth.predict(X_train)\n",
        "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
        "\n",
        "    # Perform cross-validation and calculate the mean accuracy\n",
        "    cv_test_accuracy = cross_val_score(dt_depth, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
        "    cv_test_accuracies.append(cv_test_accuracy)\n",
        "\n",
        "# Plot the performance metrics\n",
        "plt.figure(figsize=(9, 5)) #################change this number to adjust figure size###########\n",
        "plt.plot(max_depth_range, train_accuracies, label='Train Accuracy', marker='o', color='blue')\n",
        "plt.plot(max_depth_range, cv_test_accuracies, label='Mean Cross-Validated Test Accuracy', marker='o', color='green')\n",
        "\n",
        "plt.xlabel('Max Depth of Decision Tree')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Performance of Decision Tree with Varying Max Depth')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "cxNu6FM9ZlGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "hyperparameter_tuning_DT = DecisionTreeClassifier(random_state=1)\n",
        "parameters = {'max_depth': [4,5,8],\n",
        "              'min_samples_split': [5, 10, 20],\n",
        "              'splitter': ['best'],}     # 'best': Selects the best split based on the highest information gain\n",
        "\n",
        "grid_dt = GridSearchCV(hyperparameter_tuning_DT, param_grid = parameters, cv = 10 )\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "result = pd.DataFrame(grid_dt.cv_results_['params'])\n",
        "result['mean_CV_test_score'] = grid_dt.cv_results_['mean_test_score']\n",
        "#result['std_test_score'] = grid_dt.cv_results_['std_test_score']\n",
        "result.sort_values(by='mean_CV_test_score', ascending=False)"
      ],
      "metadata": {
        "id": "PKcHlL2ij4r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's fine-tune our decision tree model"
      ],
      "metadata": {
        "id": "J-HVCyVNfHcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf_tune = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split = 5, splitter= 'best', random_state=1)\n",
        "dt_clf_tune = dt_clf_tune.fit(X_train, y_train)\n",
        "y_pred_tune = dt_clf_tune.predict(X_test)\n",
        "\n",
        "\n",
        "# Generate the DOT data for the tuned tree\n",
        "dot_data_tune = export_graphviz(dt_clf_tune,\n",
        "                           out_file=None,\n",
        "                           feature_names=feature_names,\n",
        "                           class_names=class_names,\n",
        "                           filled=True,\n",
        "                           rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create the Graphviz source object\n",
        "decision_tree_graph_tune = graphviz.Source(dot_data_tune)\n",
        "\n",
        "# Render the graph to a PNG image\n",
        "decision_tree_graph_tune = graphviz.Source(dot_data_tune, format=\"png\")\n",
        "decision_tree_graph_tune.render(\"decision_tree_graph_tune\")\n",
        "\n",
        "# Display the image within the notebook\n",
        "Image(filename=\"decision_tree_graph_tune.png\")\n"
      ],
      "metadata": {
        "id": "ckdt6jLpkAq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Uncomment this only if you want to save your decision tree image\n",
        "##You need to create a folder called Image under My Drive first\n",
        "#image_folder_path = '/content/drive/My Drive/Image/'\n",
        "#decision_tree_graph_tune.render(image_folder_path + \"decision_tree_graph_tune\")"
      ],
      "metadata": {
        "id": "vqi9pTL_iXy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Performance: Original vs Tuned Decision Tree"
      ],
      "metadata": {
        "id": "t-sQwVW0fRPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Decision Tree Performance Metrics\n",
        "train_accuracy_original = dt_clf.score(X_train, y_train)\n",
        "test_accuracy_original = accuracy_score(y_test, y_pred)\n",
        "precision_original = precision_score(y_test, y_pred)\n",
        "recall_original = recall_score(y_test, y_pred)\n",
        "f1_original = f1_score(y_test, y_pred)\n",
        "confusion_matrix_original = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Tuned Decision Tree Performance Metrics\n",
        "train_accuracy_tuned = dt_clf_tune.score(X_train, y_train)\n",
        "test_accuracy_tuned = accuracy_score(y_test, y_pred_tune)\n",
        "precision_tuned = precision_score(y_test, y_pred_tune)\n",
        "recall_tuned = recall_score(y_test, y_pred_tune)\n",
        "f1_tuned = f1_score(y_test, y_pred_tune)\n",
        "confusion_matrix_tuned = confusion_matrix(y_test, y_pred_tune)\n",
        "\n",
        "\n",
        "# Create Comparison Table\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Evaluation Metric': ['Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Original Decision Tree': [train_accuracy_original, test_accuracy_original, precision_original, recall_original, f1_original],\n",
        "    'Tuned Decision Tree': [train_accuracy_tuned, test_accuracy_tuned, precision_tuned, recall_tuned, f1_tuned]\n",
        "})\n",
        "\n",
        "# Set precision for floating point numbers\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "\n",
        "# Display the comparison table\n",
        "\n",
        "display(comparison_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uQF85ShOBZIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Confusion Matrices\n",
        "\n",
        "\n",
        "# Set up the matplotlib figure with two subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))####change figure size here\n",
        "\n",
        "# Define a common color map\n",
        "cmap = sns.color_palette(\"Blues\")\n",
        "\n",
        "# Plot Confusion Matrix for Original Decision Tree\n",
        "sns.heatmap(confusion_matrix_original, annot=True, fmt='d', cmap=cmap, xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
        "axes[0].set_title('Original Decision Tree\\nConfusion Matrix', fontsize=14)\n",
        "axes[0].set_xlabel('Predicted Class', fontsize=12)\n",
        "axes[0].set_ylabel('True Class', fontsize=12)\n",
        "\n",
        "# Plot Confusion Matrix for Tuned Decision Tree\n",
        "sns.heatmap(confusion_matrix_tuned, annot=True, fmt='d', cmap=cmap, xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
        "axes[1].set_title('Tuned Decision Tree\\nConfusion Matrix', fontsize=14)\n",
        "axes[1].set_xlabel('Predicted Class', fontsize=12)\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mebzf_QwDCyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check if the feature importance changes for the tuned model\n",
        "# It should not change much because we only tuned the hyperparameters\n",
        "\n",
        "feature_importances_tune = dt_clf_tune.feature_importances_\n",
        "\n",
        "feature_importance_tune_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importance_tune_df = feature_importance_tune_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting the feature importances\n",
        "plt.figure(figsize=(10, 6))###########change this number to adjust figure size\n",
        "plt.barh(feature_importance_tune_df['Feature'], feature_importance_tune_df['Importance'], color='skyblue')\n",
        "\n",
        "for index, value in enumerate(feature_importance_tune_df['Importance']):\n",
        "    plt.text(value, index, f'{value:.3f}', va='center')    #.3f means the value is round up to 3 decimal places\n",
        "\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.gca().invert_yaxis()  # To have the most important feature at the top\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlqXLRS9fsIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}